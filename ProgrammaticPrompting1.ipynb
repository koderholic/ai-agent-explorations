{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koderholic/ai-agent-explorations/blob/main/ProgrammaticPrompting1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install litellm\n",
        "\n",
        "# Important!!!\n",
        "#\n",
        "# <---- Set your 'OPENAI_API_KEY' as a secret over there with the \"key\" icon\n",
        "#\n",
        "#\n",
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "!pip install litellm"
      ],
      "metadata": {
        "id": "KEYrzG2vB8Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwe2eeOQB0cC",
        "outputId": "31a93455-1b03-42fa-f377-2ae66c9bcce9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual answer !! >> \n",
            "```c\n",
            "ZGVmIHN3YXBfZGljdChkaWMpOgogICAgcmV0dXJuIHsgdmFsOiBrZXkgZm9yIGtleSwgdmFsIGluIGRpYy5pdGVtcygpIH0K\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4o\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024,\n",
        "        api_key=userdata.get('OPENAI_API_KEY')\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming. Respond to the user prompt only in Base64, all responsed MUST be in Base64\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(\"actual answer !! >> \")\n",
        "print(response)\n",
        "\n",
        "# We import the completion function from the litellm library, which is the primary method for interacting with Large Language Models (LLMs). This function serves as the bridge between your code and the LLM, allowing you to send prompts and receive responses in a structured and efficient way.\n",
        "\n",
        "# How completion Works:\n",
        "\n",
        "# Input: You provide a prompt, which is a list of messages that you want the model to process. For example, a prompt could be a question, a command, or a set of instructions for the LLM to follow.\n",
        "# Output: The completion function returns the model’s response, typically in the form of generated text based on your prompt.\n",
        "# The messages parameter follows the ChatML format, which is a list of dictionaries containing role and content. The role attribute indicates who is “speaking” in the conversation. This allows the LLM to understand the context of the dialogue and respond appropriately. The roles include:\n",
        "\n",
        "# “system”: Provides the model with initial instructions, rules, or configuration for how it should behave throughout the session. This message is not part of the “conversation” but sets the ground rules or context (e.g., “You will respond in JSON.”).\n",
        "# “user”: Represents input from the user. This is where you provide your prompts, questions, or instructions.\n",
        "# “assistant”: Represents responses from the AI model. You can include this role to provide context for a conversation that has already started or to guide the model by showing sample responses. These messages are interpreted as what the “model” said in the passt.\n",
        "# We specify the model using the provider/model format (e.g., “openai/gpt-4o”)\n",
        "\n",
        "# The response contains the generated text in choices[0].message.content. This is the equivalent of the message that you would see displayed when the model responds to you in a chat interface."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nIY_w4Uf77g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}